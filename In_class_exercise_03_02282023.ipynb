{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devendarreddybathini/Devendarreddy_INFO5731_spring2023/blob/main/In_class_exercise_03_02282023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YFs0qpY1x0U"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyhx2vFj1x0W"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjmJ2xUt1x0X"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NgPWjpR1x0X"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "Sorting customer reviews of a product into those with positive, negative, or neutral sentiments \n",
        "would be a fascinating text classification job. \n",
        "For businesses to comprehend customer feedback and improve their goods and services accordingly, this job is crucial.\n",
        "To build a machine learning model for this task,\n",
        "features might be useful are Bag of words, N_gram, parts of speech, sentiment lexicon and readability.\n",
        "Each word in the review is handled separately using the Bag of Words (BoW) approach.\n",
        "Each word's frequency within the evaluation is tracked and used as a feature.\n",
        "This kind of feature is helpful because it records customer vocabularies and the frequency with which \n",
        "particular words appear in favorable, unfavorable, and neutral evaluations.\n",
        "\n",
        "N-grams is similar to BoW, but it measures the frequency of contiguous sequences of N words\n",
        "rather than the frequency of individual words.\n",
        "(i.e., N-grams). Bigrams (N=2), for instance, would recognize terms like \"great product\" \n",
        "or \"poor quality\". This kind of feature is beneficial because it records how words are used in context.\n",
        "\n",
        "Part-of-speech (POS) features specify each word's grammatical function,\n",
        "such as whether it is a noun, verb, adjective, etc. \n",
        "The syntactic structure of the review and how it links to sentiment can be captured by POS features. \n",
        "\n",
        "Sentiment lexicons are collections of words that express either good or negative emotions.\n",
        "These characteristics could indicate the existence of sentimental words and their polarity in the review. \n",
        "This kind of feature is advantageous because it takes into account previous understanding of the meaning of words.\n",
        "characteristics, such as sentence length, word count per sentence, and average word length,\n",
        "reflect how complicated the language employed in the review is.\n",
        "Understanding client reading levels and how they connect to sentiment may be possible with the use of these elements.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fixsaFa1x0Y"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_-0qZ7B1x0Y",
        "outputId": "2a7b3903-24bf-42a2-f078-4347f22c41d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag of Words features:  [[0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1]\n",
            " [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 2 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1]]\n",
            "N-grams features:  [[1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1]]\n",
            "Part-of-speech (POS) features:  [['DT', 'NN', 'VBZ', 'JJ', '.', 'PRP', 'VBD', 'PRP$', 'NNS', '.', 'NN', 'MD', 'RB', 'VB', 'PRP', 'TO', 'NN', '.'], ['NN', 'VBD', 'RB', 'JJ', 'IN', 'DT', 'NN', '.', 'PRP', 'VBD', 'RB', 'VB', 'RB', 'JJ', '.'], ['RB', 'VBP', 'DT', 'NN', 'RB', 'JJ', '.', 'PRP', 'VBZ', 'VBN', 'PRP$', 'NN', 'RB', 'RB', 'JJR', '.'], ['DT', 'NN', 'VBZ', 'RB', 'RB', '.', 'PRP', 'VBD', 'RB', 'JJ', ',', 'CC', 'PRP', 'VBD', 'RB', 'JJ', 'RB', '.'], ['NN', 'MD', 'RB', 'VB', 'DT', 'NN', 'RB', '.', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']]\n",
            "Sentiment lexicon features:  [[0.458, 0.0, 0.542], [0.0, 0.253, 0.747], [0.408, 0.0, 0.592], [0.282, 0.127, 0.591], [0.0, 0.203, 0.797]]\n",
            "Readability features:  [[5.0, 5.466666666666667], [6.0, 5.25], [7.0, 3.642857142857143], [6.5, 4.769230769230769], [7.0, 4.142857142857143]]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "texts = [ \"This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.\",  \n",
        "         \"I was really disappointed with this product. It didn't work as advertised.\", \n",
        "         \"I love this product so much! It has made my life so much easier.\",  \n",
        "         \"This product is just okay. It wasn't great, but it wasn't terrible either.\", \n",
        "         \"I would never buy this product again. It was a complete waste of money.\"]\n",
        "\n",
        "# Tokenize and preprocess text data\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    processed_texts.append(' '.join(tokens))\n",
        "\n",
        "# Bag of Words features\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_features = bow_vectorizer.fit_transform(processed_texts).toarray()\n",
        "\n",
        "# N-grams features\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "ngram_features = ngram_vectorizer.fit_transform(processed_texts).toarray()\n",
        "\n",
        "# Part-of-speech (POS) features\n",
        "pos_features = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    pos_features.append([pos for token, pos in nltk.pos_tag(tokens)])\n",
        "\n",
        "# Sentiment lexicon features\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "lexicon_features = []\n",
        "for text in texts:\n",
        "    polarity_scores = sid.polarity_scores(text)\n",
        "    lexicon_features.append([polarity_scores['pos'], polarity_scores['neg'], polarity_scores['neu']])\n",
        "\n",
        "# Readability features\n",
        "readability_features = []\n",
        "for text in texts:\n",
        "    words = text.split()\n",
        "    num_words = len(words)\n",
        "    num_sentences = len(nltk.sent_tokenize(text))\n",
        "    avg_sentence_length = float(num_words/num_sentences)\n",
        "    avg_word_length = float(sum(len(word) for word in words)/num_words)\n",
        "    readability_features.append([avg_sentence_length, avg_word_length])\n",
        "\n",
        "# Print features\n",
        "print(\"Bag of Words features: \", bow_features)\n",
        "print(\"N-grams features: \", ngram_features)\n",
        "print(\"Part-of-speech (POS) features: \", pos_features)\n",
        "print(\"Sentiment lexicon features: \", lexicon_features)\n",
        "print(\"Readability features: \", readability_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouXmJEOZ1x0Z"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qs_V1-v1x0Z",
        "outputId": "e6fc1fec-938b-447b-fc3d-58483ac3b121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chi-Square scores for all features:\n",
            "smog_score 14.549999999999999\n",
            "wasn 8.000000000000002\n",
            "it wasn 8.000000000000002\n",
            "wasn 8.000000000000002\n",
            "' 4.499999999999999\n",
            "but 4.000000000000001\n",
            "either 4.000000000000001\n",
            "great 4.000000000000001\n",
            "just 4.000000000000001\n",
            "okay 4.000000000000001\n",
            "terrible 4.000000000000001\n",
            "but it 4.000000000000001\n",
            "great but 4.000000000000001\n",
            "is just 4.000000000000001\n",
            "just okay 4.000000000000001\n",
            "okay it 4.000000000000001\n",
            "terrible either 4.000000000000001\n",
            "wasn great 4.000000000000001\n",
            "wasn terrible 4.000000000000001\n",
            "great 4.000000000000001\n",
            "just 4.000000000000001\n",
            "okay 4.000000000000001\n",
            "terrible 4.000000000000001\n",
            "much 3.0\n",
            "my 3.0\n",
            "so 3.0\n",
            "was 3.0\n",
            "so much 3.0\n",
            "! 3.0\n",
            "is 1.75\n",
            "product is 1.75\n",
            "advertised 1.5\n",
            "again 1.5\n",
            "amazing 1.5\n",
            "anyone 1.5\n",
            "as 1.5\n",
            "buy 1.5\n",
            "complete 1.5\n",
            "definitely 1.5\n",
            "didn 1.5\n",
            "disappointed 1.5\n",
            "easier 1.5\n",
            "exceeded 1.5\n",
            "expectations 1.5\n",
            "has 1.5\n",
            "life 1.5\n",
            "love 1.5\n",
            "made 1.5\n",
            "money 1.5\n",
            "never 1.5\n",
            "of 1.5\n",
            "really 1.5\n",
            "recommend 1.5\n",
            "to 1.5\n",
            "waste 1.5\n",
            "with 1.5\n",
            "work 1.5\n",
            "again it 1.5\n",
            "amazing it 1.5\n",
            "as advertised 1.5\n",
            "buy this 1.5\n",
            "complete waste 1.5\n",
            "definitely recommend 1.5\n",
            "didn work 1.5\n",
            "disappointed with 1.5\n",
            "exceeded my 1.5\n",
            "expectations would 1.5\n",
            "has made 1.5\n",
            "is amazing 1.5\n",
            "it didn 1.5\n",
            "it exceeded 1.5\n",
            "it has 1.5\n",
            "it to 1.5\n",
            "it was 1.5\n",
            "life so 1.5\n",
            "love this 1.5\n",
            "made my 1.5\n",
            "much easier 1.5\n",
            "much it 1.5\n",
            "my expectations 1.5\n",
            "my life 1.5\n",
            "never buy 1.5\n",
            "of money 1.5\n",
            "product again 1.5\n",
            "product it 1.5\n",
            "product so 1.5\n",
            "really disappointed 1.5\n",
            "recommend it 1.5\n",
            "to anyone 1.5\n",
            "was complete 1.5\n",
            "was really 1.5\n",
            "waste of 1.5\n",
            "with this 1.5\n",
            "work as 1.5\n",
            "would definitely 1.5\n",
            "would never 1.5\n",
            "advertised 1.5\n",
            "amazing 1.5\n",
            "buy 1.5\n",
            "complete 1.5\n",
            "definitely 1.5\n",
            "didn 1.5\n",
            "disappointed 1.5\n",
            "easier 1.5\n",
            "exceeded 1.5\n",
            "expectations 1.5\n",
            "life 1.5\n",
            "love 1.5\n",
            "money 1.5\n",
            "really 1.5\n",
            "recommend 1.5\n",
            "waste 1.5\n",
            "work 1.5\n",
            "pos 0.7872556561085972\n",
            "would 0.5\n",
            "it 0.49999999999999994\n",
            "neg 0.3990138888888888\n",
            "compound 0.3129373803350619\n",
            "flesch_score 0.18403360985204875\n",
            "neu 0.07455290611028313\n",
            "product 0.0\n",
            "this 0.0\n",
            "this product 0.0\n",
            "product 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\bdeve\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "#using chi square\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "import textstat\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Sample data\n",
        "documents= [\"This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.\", \n",
        "            \"I was really disappointed with this product. It didn't work as advertised.\", \n",
        "            \"I love this product so much! It has made my life so much easier.\",    \n",
        "            \"This product is just okay. It wasn't great, but it wasn't terrible either.\",  \n",
        "            \"I would never buy this product again. It was a complete waste of money.\"]\n",
        "\n",
        "# Target class labels\n",
        "labels = np.array([1, 0, 1,2,0])  # 1 for positive, 0 for negative, 2 for neutral\n",
        "\n",
        "# Bag of Words feature extraction\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_features = vectorizer_bow.fit_transform(documents)\n",
        "\n",
        "# N-Grams feature extraction\n",
        "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram_features = vectorizer_ngrams.fit_transform(documents)\n",
        "\n",
        "# Part-of-speech feature extraction\n",
        "pos_vectorizer = CountVectorizer(token_pattern=r'\\b\\w\\w+\\b|!|\\?|\\\"|\\'', ngram_range=(1,1), analyzer='word', \n",
        "                                 stop_words='english')\n",
        "pos_features = pos_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Sentiment Lexicon feature extraction\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "lexicon_features = []\n",
        "for doc in documents:\n",
        "    vs = analyzer.polarity_scores(doc)\n",
        "    # apply non-negative transformation\n",
        "    lexicon_features.append([abs(vs['neg']), abs(vs['neu']), abs(vs['pos']), abs(vs['compound'])])\n",
        "lexicon_features = np.array(lexicon_features)\n",
        "\n",
        "# Readability feature extraction\n",
        "readability_features = []\n",
        "for doc in documents:\n",
        "    flesch_score = textstat.flesch_reading_ease(doc)\n",
        "    smog_score = textstat.smog_index(doc)\n",
        "    # apply non-negative transformation\n",
        "    readability_features.append([abs(flesch_score), abs(smog_score)])\n",
        "readability_features = np.array(readability_features)\n",
        "\n",
        "# Concatenate all features horizontally\n",
        "features = hstack((bow_features, ngram_features, pos_features, lexicon_features, readability_features))\n",
        "\n",
        "# Get feature names\n",
        "feature_names = vectorizer_bow.get_feature_names() + vectorizer_ngrams.get_feature_names() + pos_vectorizer.get_feature_names() + ['neg', 'neu', 'pos', 'compound'] + ['flesch_score', 'smog_score']\n",
        "\n",
        "# Chi-Square feature selection\n",
        "chi2_scores, _ = chi2(features, labels)\n",
        "feature_scores = list(zip(feature_names, chi2_scores))\n",
        "feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"Chi-Square scores for all features:\")\n",
        "for feature, score in feature_scores:\n",
        "    print(feature, score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6bnc2wu1x0Z"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGXWzHHY1x0a",
        "outputId": "b6d46c08-ede6-4eb1-98cf-0f408a42002e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\"This product is just okay. It wasn't great, but it wasn't terrible either.\", 0.60150015), ('I love this product so much! It has made my life so much easier.', 0.56847817), ('This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.', 0.45820385), (\"I was really disappointed with this product. It didn't work as advertised.\", 0.44249266), ('I would never buy this product again. It was a complete waste of money.', 0.29375678)]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Define the query and texts\n",
        "#provding almost a neutral statement as query\n",
        "query = \"I recently tried out the new restaurant in town and found the food to be decent.The service was good and the ambiance was pleasant. However, the prices were a bit on the higher side. Overall, it was a decent experience but I am not sure if I would go back again given the price point.\"\n",
        "data = [\"This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.\", \n",
        "            \"I was really disappointed with this product. It didn't work as advertised.\", \n",
        "            \"I love this product so much! It has made my life so much easier.\",    \n",
        "            \"This product is just okay. It wasn't great, but it wasn't terrible either.\",  \n",
        "            \"I would never buy this product again. It was a complete waste of money.\"]\n",
        "\n",
        "\n",
        "# Compute the embeddings for the query and texts\n",
        "query_embedding = model.encode([query])[0]\n",
        "text_embeddings = model.encode(texts)\n",
        "\n",
        "# Compute the cosine similarities between the query and texts\n",
        "similarities = cosine_similarity([query_embedding], text_embeddings)[0]\n",
        "\n",
        "# Rank the texts based on their similarity to the query\n",
        "ranked_texts = sorted(zip(texts, similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(ranked_texts)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}